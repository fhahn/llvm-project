; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; RUN: opt -passes=loop-vectorize -force-vector-width=4 -S %s | FileCheck %s
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"

define void @replace_first_order_recurrence_with_versioned_iv_for_pointer_use(ptr %y, ptr %x, i32 %n) {
; CHECK-LABEL: define void @replace_first_order_recurrence_with_versioned_iv_for_pointer_use(
; CHECK-SAME: ptr [[Y:%.*]], ptr [[X:%.*]], i32 [[N:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    [[X2:%.*]] = ptrtoint ptr [[X]] to i64
; CHECK-NEXT:    [[Y1:%.*]] = ptrtoint ptr [[Y]] to i64
; CHECK-NEXT:    [[SMAX:%.*]] = call i32 @llvm.smax.i32(i32 [[N]], i32 1)
; CHECK-NEXT:    [[TMP0:%.*]] = zext nneg i32 [[SMAX]] to i64
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[TMP0]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[SCALAR_PH:.*]], label %[[VECTOR_MEMCHECK:.*]]
; CHECK:       [[VECTOR_MEMCHECK]]:
; CHECK-NEXT:    [[TMP8:%.*]] = add i64 [[Y1]], -8
; CHECK-NEXT:    [[TMP9:%.*]] = sub i64 [[TMP8]], [[X2]]
; CHECK-NEXT:    [[DIFF_CHECK:%.*]] = icmp ult i64 [[TMP9]], 32
; CHECK-NEXT:    br i1 [[DIFF_CHECK]], label %[[SCALAR_PH]], label %[[VECTOR_PH:.*]]
; CHECK:       [[VECTOR_PH]]:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[TMP0]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[TMP0]], [[N_MOD_VF]]
; CHECK-NEXT:    [[IND_END:%.*]] = trunc i64 [[N_VEC]] to i32
; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
; CHECK:       [[VECTOR_BODY]]:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-NEXT:    [[OFFSET_IDX:%.*]] = trunc i64 [[INDEX]] to i32
; CHECK-NEXT:    [[TMP3:%.*]] = add i32 [[OFFSET_IDX]], 0
; CHECK-NEXT:    [[TMP32:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP5:%.*]] = add i32 [[TMP3]], 1
; CHECK-NEXT:    [[TMP22:%.*]] = zext i32 [[TMP5]] to i64
; CHECK-NEXT:    [[TMP29:%.*]] = getelementptr inbounds double, ptr [[X]], i64 [[TMP22]]
; CHECK-NEXT:    [[TMP33:%.*]] = getelementptr inbounds double, ptr [[Y]], i64 [[TMP32]]
; CHECK-NEXT:    [[TMP17:%.*]] = getelementptr inbounds double, ptr [[TMP29]], i32 0
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x double>, ptr [[TMP17]], align 8
; CHECK-NEXT:    [[TMP18:%.*]] = getelementptr inbounds double, ptr [[TMP33]], i32 0
; CHECK-NEXT:    store <4 x double> [[WIDE_LOAD]], ptr [[TMP18]], align 8
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP11:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP11]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
; CHECK:       [[MIDDLE_BLOCK]]:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[TMP0]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label %[[FOR_END:.*]], label %[[SCALAR_PH]]
; CHECK:       [[SCALAR_PH]]:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i32 [ [[IND_END]], %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ], [ 0, %[[VECTOR_MEMCHECK]] ]
; CHECK-NEXT:    [[BC_RESUME_VAL3:%.*]] = phi i64 [ [[N_VEC]], %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ], [ 0, %[[VECTOR_MEMCHECK]] ]
; CHECK-NEXT:    br label %[[FOR_BODY:.*]]
; CHECK:       [[FOR_BODY]]:
; CHECK-NEXT:    [[PHI32:%.*]] = phi i32 [ [[BC_RESUME_VAL]], %[[SCALAR_PH]] ], [ [[I32NEXT:%.*]], %[[FOR_BODY]] ]
; CHECK-NEXT:    [[SCALAR_RECUR:%.*]] = phi i64 [ [[BC_RESUME_VAL3]], %[[SCALAR_PH]] ], [ [[I64NEXT:%.*]], %[[FOR_BODY]] ]
; CHECK-NEXT:    [[I32NEXT]] = add i32 [[PHI32]], 1
; CHECK-NEXT:    [[I64NEXT]] = zext i32 [[I32NEXT]] to i64
; CHECK-NEXT:    [[XIP:%.*]] = getelementptr inbounds double, ptr [[X]], i64 [[I64NEXT]]
; CHECK-NEXT:    [[YIP:%.*]] = getelementptr inbounds double, ptr [[Y]], i64 [[SCALAR_RECUR]]
; CHECK-NEXT:    [[XI:%.*]] = load double, ptr [[XIP]], align 8
; CHECK-NEXT:    store double [[XI]], ptr [[YIP]], align 8
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[I32NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[CMP]], label %[[FOR_BODY]], label %[[FOR_END]], !llvm.loop [[LOOP3:![0-9]+]]
; CHECK:       [[FOR_END]]:
; CHECK-NEXT:    ret void
;
entry:
  br label %for.body

for.body:
  %phi32 = phi i32 [ 0, %entry ], [ %i32next, %for.body ]
  %phi64 = phi i64 [ 0, %entry ], [ %i64next, %for.body ]
  %i32next = add i32 %phi32, 1
  %i64next = zext i32 %i32next to i64
  %xip = getelementptr inbounds double, ptr %x, i64 %i64next
  %yip = getelementptr inbounds double, ptr %y, i64 %phi64
  %xi = load double, ptr %xip, align 8
  store double %xi, ptr %yip, align 8
  %cmp = icmp slt i32 %i32next, %n
  br i1 %cmp, label %for.body, label %for.end

for.end:
  ret void
}

define void @replace_first_order_recurrence_with_versioned_iv_for_pointer_use_btc_versioned(ptr %y, ptr %x, i64 %n) {
; CHECK-LABEL: define void @replace_first_order_recurrence_with_versioned_iv_for_pointer_use_btc_versioned(
; CHECK-SAME: ptr [[Y:%.*]], ptr [[X:%.*]], i64 [[N:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    [[X2:%.*]] = ptrtoint ptr [[X]] to i64
; CHECK-NEXT:    [[Y1:%.*]] = ptrtoint ptr [[Y]] to i64
; CHECK-NEXT:    [[SMAX3:%.*]] = call i64 @llvm.smax.i64(i64 [[N]], i64 1)
; CHECK-NEXT:    [[TMP0:%.*]] = trunc i64 [[SMAX3]] to i32
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i32 [[TMP0]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[SCALAR_PH:.*]], label %[[VECTOR_SCEVCHECK:.*]]
; CHECK:       [[VECTOR_SCEVCHECK]]:
; CHECK-NEXT:    [[SMAX:%.*]] = call i64 @llvm.smax.i64(i64 [[N]], i64 1)
; CHECK-NEXT:    [[TMP1:%.*]] = add nsw i64 [[SMAX]], -1
; CHECK-NEXT:    [[TMP2:%.*]] = trunc i64 [[TMP1]] to i32
; CHECK-NEXT:    [[TMP3:%.*]] = add i32 1, [[TMP2]]
; CHECK-NEXT:    [[TMP4:%.*]] = icmp ult i32 [[TMP3]], 1
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ugt i64 [[TMP1]], 4294967295
; CHECK-NEXT:    [[TMP6:%.*]] = or i1 [[TMP4]], [[TMP5]]
; CHECK-NEXT:    [[TMP7:%.*]] = icmp ugt i64 [[TMP1]], 4294967295
; CHECK-NEXT:    br i1 [[TMP6]], label %[[SCALAR_PH]], label %[[VECTOR_MEMCHECK:.*]]
; CHECK:       [[VECTOR_MEMCHECK]]:
; CHECK-NEXT:    [[TMP8:%.*]] = add i64 [[Y1]], -8
; CHECK-NEXT:    [[TMP9:%.*]] = sub i64 [[TMP8]], [[X2]]
; CHECK-NEXT:    [[DIFF_CHECK:%.*]] = icmp ult i64 [[TMP9]], 32
; CHECK-NEXT:    br i1 [[DIFF_CHECK]], label %[[SCALAR_PH]], label %[[VECTOR_PH:.*]]
; CHECK:       [[VECTOR_PH]]:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i32 [[TMP0]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i32 [[TMP0]], [[N_MOD_VF]]
; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
; CHECK:       [[VECTOR_BODY]]:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i32 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_IND:%.*]] = phi <4 x i32> [ <i32 0, i32 1, i32 2, i32 3>, %[[VECTOR_PH]] ], [ [[VEC_IND_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VECTOR_RECUR:%.*]] = phi <4 x i64> [ <i64 poison, i64 poison, i64 poison, i64 0>, %[[VECTOR_PH]] ], [ [[TMP11:%.*]], %[[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP10:%.*]] = add <4 x i32> [[VEC_IND]], <i32 1, i32 1, i32 1, i32 1>
; CHECK-NEXT:    [[TMP11]] = zext <4 x i32> [[TMP10]] to <4 x i64>
; CHECK-NEXT:    [[TMP12:%.*]] = shufflevector <4 x i64> [[VECTOR_RECUR]], <4 x i64> [[TMP11]], <4 x i32> <i32 3, i32 4, i32 5, i32 6>
; CHECK-NEXT:    [[TMP13:%.*]] = extractelement <4 x i64> [[TMP11]], i32 0
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds double, ptr [[X]], i64 [[TMP13]]
; CHECK-NEXT:    [[TMP15:%.*]] = extractelement <4 x i64> [[TMP12]], i32 0
; CHECK-NEXT:    [[TMP16:%.*]] = getelementptr inbounds double, ptr [[Y]], i64 [[TMP15]]
; CHECK-NEXT:    [[TMP17:%.*]] = getelementptr inbounds double, ptr [[TMP14]], i32 0
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x double>, ptr [[TMP17]], align 8
; CHECK-NEXT:    [[TMP18:%.*]] = getelementptr inbounds double, ptr [[TMP16]], i32 0
; CHECK-NEXT:    store <4 x double> [[WIDE_LOAD]], ptr [[TMP18]], align 8
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i32 [[INDEX]], 4
; CHECK-NEXT:    [[VEC_IND_NEXT]] = add <4 x i32> [[VEC_IND]], <i32 4, i32 4, i32 4, i32 4>
; CHECK-NEXT:    [[TMP19:%.*]] = icmp eq i32 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP19]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP4:![0-9]+]]
; CHECK:       [[MIDDLE_BLOCK]]:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i32 [[TMP0]], [[N_VEC]]
; CHECK-NEXT:    [[VECTOR_RECUR_EXTRACT:%.*]] = extractelement <4 x i64> [[TMP11]], i32 3
; CHECK-NEXT:    br i1 [[CMP_N]], label %[[FOR_END:.*]], label %[[SCALAR_PH]]
; CHECK:       [[SCALAR_PH]]:
; CHECK-NEXT:    [[SCALAR_RECUR_INIT:%.*]] = phi i64 [ 0, %[[VECTOR_MEMCHECK]] ], [ 0, %[[VECTOR_SCEVCHECK]] ], [ 0, %[[ENTRY]] ], [ [[VECTOR_RECUR_EXTRACT]], %[[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i32 [ [[N_VEC]], %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ], [ 0, %[[VECTOR_SCEVCHECK]] ], [ 0, %[[VECTOR_MEMCHECK]] ]
; CHECK-NEXT:    br label %[[FOR_BODY:.*]]
; CHECK:       [[FOR_BODY]]:
; CHECK-NEXT:    [[PHI32:%.*]] = phi i32 [ [[BC_RESUME_VAL]], %[[SCALAR_PH]] ], [ [[I32NEXT:%.*]], %[[FOR_BODY]] ]
; CHECK-NEXT:    [[SCALAR_RECUR:%.*]] = phi i64 [ [[SCALAR_RECUR_INIT]], %[[SCALAR_PH]] ], [ [[I64NEXT:%.*]], %[[FOR_BODY]] ]
; CHECK-NEXT:    [[I32NEXT]] = add i32 [[PHI32]], 1
; CHECK-NEXT:    [[I64NEXT]] = zext i32 [[I32NEXT]] to i64
; CHECK-NEXT:    [[XIP:%.*]] = getelementptr inbounds double, ptr [[X]], i64 [[I64NEXT]]
; CHECK-NEXT:    [[YIP:%.*]] = getelementptr inbounds double, ptr [[Y]], i64 [[SCALAR_RECUR]]
; CHECK-NEXT:    [[XI:%.*]] = load double, ptr [[XIP]], align 8
; CHECK-NEXT:    store double [[XI]], ptr [[YIP]], align 8
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i64 [[I64NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[CMP]], label %[[FOR_BODY]], label %[[FOR_END]], !llvm.loop [[LOOP5:![0-9]+]]
; CHECK:       [[FOR_END]]:
; CHECK-NEXT:    ret void
;
entry:
  br label %for.body

for.body:
  %phi32 = phi i32 [ 0, %entry ], [ %i32next, %for.body ]
  %phi64 = phi i64 [ 0, %entry ], [ %i64next, %for.body ]
  %i32next = add i32 %phi32, 1
  %i64next = zext i32 %i32next to i64
  %xip = getelementptr inbounds double, ptr %x, i64 %i64next
  %yip = getelementptr inbounds double, ptr %y, i64 %phi64
  %xi = load double, ptr %xip, align 8
  store double %xi, ptr %yip, align 8
  %cmp = icmp slt i64 %i64next, %n
  br i1 %cmp, label %for.body, label %for.end

for.end:
  ret void
}

define void @do_not_replace_first_order_recurrence_with_versioned_iv_for_wide_pointer_use(ptr %y, ptr %x, i32 %n) {
; CHECK-LABEL: define void @do_not_replace_first_order_recurrence_with_versioned_iv_for_wide_pointer_use(
; CHECK-SAME: ptr [[Y:%.*]], ptr [[X:%.*]], i32 [[N:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    [[X2:%.*]] = ptrtoint ptr [[X]] to i64
; CHECK-NEXT:    [[Y1:%.*]] = ptrtoint ptr [[Y]] to i64
; CHECK-NEXT:    [[SMAX:%.*]] = call i32 @llvm.smax.i32(i32 [[N]], i32 1)
; CHECK-NEXT:    [[TMP0:%.*]] = zext nneg i32 [[SMAX]] to i64
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[TMP0]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[SCALAR_PH:.*]], label %[[VECTOR_MEMCHECK:.*]]
; CHECK:       [[VECTOR_MEMCHECK]]:
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[Y1]], -8
; CHECK-NEXT:    [[TMP2:%.*]] = sub i64 [[TMP1]], [[X2]]
; CHECK-NEXT:    [[DIFF_CHECK:%.*]] = icmp ult i64 [[TMP2]], 32
; CHECK-NEXT:    br i1 [[DIFF_CHECK]], label %[[SCALAR_PH]], label %[[VECTOR_PH:.*]]
; CHECK:       [[VECTOR_PH]]:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[TMP0]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[TMP0]], [[N_MOD_VF]]
; CHECK-NEXT:    [[IND_END:%.*]] = trunc i64 [[N_VEC]] to i32
; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
; CHECK:       [[VECTOR_BODY]]:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_IND:%.*]] = phi <4 x i32> [ <i32 0, i32 1, i32 2, i32 3>, %[[VECTOR_PH]] ], [ [[VEC_IND_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-NEXT:    [[OFFSET_IDX:%.*]] = trunc i64 [[INDEX]] to i32
; CHECK-NEXT:    [[TMP3:%.*]] = add i32 [[OFFSET_IDX]], 0
; CHECK-NEXT:    [[TMP4:%.*]] = add <4 x i32> [[VEC_IND]], <i32 1, i32 1, i32 1, i32 1>
; CHECK-NEXT:    [[TMP5:%.*]] = zext <4 x i32> [[TMP4]] to <4 x i64>
; CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i32> [[TMP4]], i32 0
; CHECK-NEXT:    [[TMP7:%.*]] = getelementptr inbounds i64, ptr [[X]], i32 [[TMP6]]
; CHECK-NEXT:    [[TMP8:%.*]] = getelementptr inbounds i64, ptr [[Y]], i32 [[TMP3]]
; CHECK-NEXT:    [[TMP9:%.*]] = getelementptr inbounds i64, ptr [[TMP7]], i32 0
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x i64>, ptr [[TMP9]], align 8
; CHECK-NEXT:    [[TMP10:%.*]] = add <4 x i64> [[WIDE_LOAD]], [[TMP5]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds i64, ptr [[TMP8]], i32 0
; CHECK-NEXT:    store <4 x i64> [[TMP10]], ptr [[TMP11]], align 8
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[VEC_IND_NEXT]] = add <4 x i32> [[VEC_IND]], <i32 4, i32 4, i32 4, i32 4>
; CHECK-NEXT:    [[TMP12:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP12]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP6:![0-9]+]]
; CHECK:       [[MIDDLE_BLOCK]]:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[TMP0]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label %[[FOR_END:.*]], label %[[SCALAR_PH]]
; CHECK:       [[SCALAR_PH]]:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i32 [ [[IND_END]], %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ], [ 0, %[[VECTOR_MEMCHECK]] ]
; CHECK-NEXT:    [[BC_RESUME_VAL3:%.*]] = phi i64 [ [[N_VEC]], %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ], [ 0, %[[VECTOR_MEMCHECK]] ]
; CHECK-NEXT:    br label %[[FOR_BODY:.*]]
; CHECK:       [[FOR_BODY]]:
; CHECK-NEXT:    [[PHI32:%.*]] = phi i32 [ [[BC_RESUME_VAL]], %[[SCALAR_PH]] ], [ [[I32NEXT:%.*]], %[[FOR_BODY]] ]
; CHECK-NEXT:    [[PHI64:%.*]] = phi i64 [ [[BC_RESUME_VAL3]], %[[SCALAR_PH]] ], [ [[I64NEXT:%.*]], %[[FOR_BODY]] ]
; CHECK-NEXT:    [[I32NEXT]] = add i32 [[PHI32]], 1
; CHECK-NEXT:    [[I64NEXT]] = zext i32 [[I32NEXT]] to i64
; CHECK-NEXT:    [[XIP:%.*]] = getelementptr inbounds i64, ptr [[X]], i32 [[I32NEXT]]
; CHECK-NEXT:    [[YIP:%.*]] = getelementptr inbounds i64, ptr [[Y]], i32 [[PHI32]]
; CHECK-NEXT:    [[XI:%.*]] = load i64, ptr [[XIP]], align 8
; CHECK-NEXT:    [[ADD:%.*]] = add i64 [[XI]], [[I64NEXT]]
; CHECK-NEXT:    store i64 [[ADD]], ptr [[YIP]], align 8
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[I32NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[CMP]], label %[[FOR_BODY]], label %[[FOR_END]], !llvm.loop [[LOOP7:![0-9]+]]
; CHECK:       [[FOR_END]]:
; CHECK-NEXT:    ret void
;
entry:
  br label %for.body

for.body:
  %phi32 = phi i32 [ 0, %entry ], [ %i32next, %for.body ]
  %phi64 = phi i64 [ 0, %entry ], [ %i64next, %for.body ]
  %i32next = add i32 %phi32, 1
  %i64next = zext i32 %i32next to i64
  %xip = getelementptr inbounds i64, ptr %x, i32 %i32next
  %yip = getelementptr inbounds i64, ptr %y, i32 %phi32
  %xi = load i64, ptr %xip, align 8
  %add = add i64 %xi, %i64next
  store i64 %add, ptr %yip, align 8
  %cmp = icmp slt i32 %i32next, %n
  br i1 %cmp, label %for.body, label %for.end

for.end:
  ret void
}

define void @do_not_replace_first_order_recurrence_with_versioned_iv_for_uniform_non_pointer_use(ptr %y, ptr %x, i64 %n) {
; CHECK-LABEL: define void @do_not_replace_first_order_recurrence_with_versioned_iv_for_uniform_non_pointer_use(
; CHECK-SAME: ptr [[Y:%.*]], ptr [[X:%.*]], i64 [[N:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    [[X2:%.*]] = ptrtoint ptr [[X]] to i64
; CHECK-NEXT:    [[Y1:%.*]] = ptrtoint ptr [[Y]] to i64
; CHECK-NEXT:    [[SMAX3:%.*]] = call i64 @llvm.smax.i64(i64 [[N]], i64 1)
; CHECK-NEXT:    [[TMP0:%.*]] = trunc i64 [[SMAX3]] to i32
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i32 [[TMP0]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[SCALAR_PH:.*]], label %[[VECTOR_SCEVCHECK:.*]]
; CHECK:       [[VECTOR_SCEVCHECK]]:
; CHECK-NEXT:    [[SMAX:%.*]] = call i64 @llvm.smax.i64(i64 [[N]], i64 1)
; CHECK-NEXT:    [[TMP1:%.*]] = add nsw i64 [[SMAX]], -1
; CHECK-NEXT:    [[TMP2:%.*]] = trunc i64 [[TMP1]] to i32
; CHECK-NEXT:    [[TMP3:%.*]] = add i32 1, [[TMP2]]
; CHECK-NEXT:    [[TMP4:%.*]] = icmp ult i32 [[TMP3]], 1
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ugt i64 [[TMP1]], 4294967295
; CHECK-NEXT:    [[TMP6:%.*]] = or i1 [[TMP4]], [[TMP5]]
; CHECK-NEXT:    [[TMP7:%.*]] = trunc i64 [[TMP1]] to i32
; CHECK-NEXT:    [[TMP8:%.*]] = icmp slt i32 [[TMP7]], 0
; CHECK-NEXT:    [[TMP9:%.*]] = icmp ugt i64 [[TMP1]], 4294967295
; CHECK-NEXT:    [[TMP10:%.*]] = or i1 [[TMP8]], [[TMP9]]
; CHECK-NEXT:    [[TMP11:%.*]] = trunc i64 [[TMP1]] to i32
; CHECK-NEXT:    [[TMP12:%.*]] = add i32 1, [[TMP11]]
; CHECK-NEXT:    [[TMP13:%.*]] = icmp slt i32 [[TMP12]], 1
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ugt i64 [[TMP1]], 4294967295
; CHECK-NEXT:    [[TMP15:%.*]] = or i1 [[TMP13]], [[TMP14]]
; CHECK-NEXT:    [[TMP16:%.*]] = or i1 [[TMP6]], [[TMP10]]
; CHECK-NEXT:    [[TMP17:%.*]] = or i1 [[TMP16]], [[TMP15]]
; CHECK-NEXT:    br i1 [[TMP17]], label %[[SCALAR_PH]], label %[[VECTOR_MEMCHECK:.*]]
; CHECK:       [[VECTOR_MEMCHECK]]:
; CHECK-NEXT:    [[TMP18:%.*]] = add i64 [[Y1]], -8
; CHECK-NEXT:    [[TMP19:%.*]] = sub i64 [[TMP18]], [[X2]]
; CHECK-NEXT:    [[DIFF_CHECK:%.*]] = icmp ult i64 [[TMP19]], 32
; CHECK-NEXT:    br i1 [[DIFF_CHECK]], label %[[SCALAR_PH]], label %[[VECTOR_PH:.*]]
; CHECK:       [[VECTOR_PH]]:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i32 [[TMP0]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i32 [[TMP0]], [[N_MOD_VF]]
; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
; CHECK:       [[VECTOR_BODY]]:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i32 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_IND:%.*]] = phi <4 x i32> [ <i32 0, i32 1, i32 2, i32 3>, %[[VECTOR_PH]] ], [ [[VEC_IND_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VECTOR_RECUR:%.*]] = phi <4 x i64> [ <i64 poison, i64 poison, i64 poison, i64 0>, %[[VECTOR_PH]] ], [ [[TMP22:%.*]], %[[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP20:%.*]] = add i32 [[INDEX]], 0
; CHECK-NEXT:    [[TMP21:%.*]] = add <4 x i32> [[VEC_IND]], <i32 1, i32 1, i32 1, i32 1>
; CHECK-NEXT:    [[TMP22]] = zext <4 x i32> [[TMP21]] to <4 x i64>
; CHECK-NEXT:    [[TMP23:%.*]] = shufflevector <4 x i64> [[VECTOR_RECUR]], <4 x i64> [[TMP22]], <4 x i32> <i32 3, i32 4, i32 5, i32 6>
; CHECK-NEXT:    [[TMP24:%.*]] = extractelement <4 x i32> [[TMP21]], i32 0
; CHECK-NEXT:    [[TMP25:%.*]] = getelementptr inbounds double, ptr [[X]], i32 [[TMP24]]
; CHECK-NEXT:    [[TMP26:%.*]] = getelementptr inbounds double, ptr [[Y]], i32 [[TMP20]]
; CHECK-NEXT:    [[TMP27:%.*]] = getelementptr inbounds double, ptr [[TMP25]], i32 0
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x double>, ptr [[TMP27]], align 8
; CHECK-NEXT:    [[TMP28:%.*]] = getelementptr inbounds double, ptr [[TMP26]], i32 0
; CHECK-NEXT:    store <4 x double> [[WIDE_LOAD]], ptr [[TMP28]], align 8
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i32 [[INDEX]], 4
; CHECK-NEXT:    [[VEC_IND_NEXT]] = add <4 x i32> [[VEC_IND]], <i32 4, i32 4, i32 4, i32 4>
; CHECK-NEXT:    [[TMP29:%.*]] = icmp eq i32 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP29]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP8:![0-9]+]]
; CHECK:       [[MIDDLE_BLOCK]]:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i32 [[TMP0]], [[N_VEC]]
; CHECK-NEXT:    [[VECTOR_RECUR_EXTRACT:%.*]] = extractelement <4 x i64> [[TMP22]], i32 3
; CHECK-NEXT:    br i1 [[CMP_N]], label %[[FOR_END:.*]], label %[[SCALAR_PH]]
; CHECK:       [[SCALAR_PH]]:
; CHECK-NEXT:    [[SCALAR_RECUR_INIT:%.*]] = phi i64 [ 0, %[[VECTOR_MEMCHECK]] ], [ 0, %[[VECTOR_SCEVCHECK]] ], [ 0, %[[ENTRY]] ], [ [[VECTOR_RECUR_EXTRACT]], %[[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i32 [ [[N_VEC]], %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ], [ 0, %[[VECTOR_SCEVCHECK]] ], [ 0, %[[VECTOR_MEMCHECK]] ]
; CHECK-NEXT:    br label %[[FOR_BODY:.*]]
; CHECK:       [[FOR_BODY]]:
; CHECK-NEXT:    [[PHI32:%.*]] = phi i32 [ [[BC_RESUME_VAL]], %[[SCALAR_PH]] ], [ [[I32NEXT:%.*]], %[[FOR_BODY]] ]
; CHECK-NEXT:    [[SCALAR_RECUR:%.*]] = phi i64 [ [[SCALAR_RECUR_INIT]], %[[SCALAR_PH]] ], [ [[I64NEXT:%.*]], %[[FOR_BODY]] ]
; CHECK-NEXT:    [[I32NEXT]] = add i32 [[PHI32]], 1
; CHECK-NEXT:    [[I64NEXT]] = zext i32 [[I32NEXT]] to i64
; CHECK-NEXT:    [[XIP:%.*]] = getelementptr inbounds double, ptr [[X]], i32 [[I32NEXT]]
; CHECK-NEXT:    [[YIP:%.*]] = getelementptr inbounds double, ptr [[Y]], i32 [[PHI32]]
; CHECK-NEXT:    [[XI:%.*]] = load double, ptr [[XIP]], align 8
; CHECK-NEXT:    store double [[XI]], ptr [[YIP]], align 8
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i64 [[I64NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[CMP]], label %[[FOR_BODY]], label %[[FOR_END]], !llvm.loop [[LOOP9:![0-9]+]]
; CHECK:       [[FOR_END]]:
; CHECK-NEXT:    ret void
;
entry:
  br label %for.body

for.body:
  %phi32 = phi i32 [ 0, %entry ], [ %i32next, %for.body ]
  %phi64 = phi i64 [ 0, %entry ], [ %i64next, %for.body ]
  %i32next = add i32 %phi32, 1
  %i64next = zext i32 %i32next to i64
  %xip = getelementptr inbounds double, ptr %x, i32 %i32next
  %yip = getelementptr inbounds double, ptr %y, i32 %phi32
  %xi = load double, ptr %xip, align 8
  store double %xi, ptr %yip, align 8
  %cmp = icmp slt i64 %i64next, %n
  br i1 %cmp, label %for.body, label %for.end

for.end:
  ret void
}
;.
; CHECK: [[LOOP0]] = distinct !{[[LOOP0]], [[META1:![0-9]+]], [[META2:![0-9]+]]}
; CHECK: [[META1]] = !{!"llvm.loop.isvectorized", i32 1}
; CHECK: [[META2]] = !{!"llvm.loop.unroll.runtime.disable"}
; CHECK: [[LOOP3]] = distinct !{[[LOOP3]], [[META1]]}
; CHECK: [[LOOP4]] = distinct !{[[LOOP4]], [[META1]], [[META2]]}
; CHECK: [[LOOP5]] = distinct !{[[LOOP5]], [[META1]]}
; CHECK: [[LOOP6]] = distinct !{[[LOOP6]], [[META1]], [[META2]]}
; CHECK: [[LOOP7]] = distinct !{[[LOOP7]], [[META1]]}
; CHECK: [[LOOP8]] = distinct !{[[LOOP8]], [[META1]], [[META2]]}
; CHECK: [[LOOP9]] = distinct !{[[LOOP9]], [[META1]]}
;.
